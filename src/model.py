import requests
import logging
import os
import zipfile
import json
import re
import shutil
from io import BytesIO
from datetime import datetime
from shiny import ui
import cv2
import numpy as np
import time as tm
import multiprocessing
from src.baseconfig import CONFIG, set_language, update_single_config_parameter
from src.camera import videostream, image_buffer, VideoStream, DetectedObject
from src.helper import sigterm_monitor, get_timezone, is_valid_uuid4
from src.database import get_cat_names_list

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from tflite_runtime.interpreter import Interpreter

_ = set_language(CONFIG['LANGUAGE'])

class RemoteModelTrainer:
    BASE_URL = "https://kittyhack-models.fk-cloud.de"

    @staticmethod
    def enqueue_model_training(zip_file_path, model_name = "", user_name = "", email = ""):
        """
        Uploads a zip file to the remote server to start model training.
        Returns the job_id on success.
        """
        url = f"{RemoteModelTrainer.BASE_URL}/upload"
        files = {'file': open(zip_file_path, 'rb')}
        data = {
            'username': user_name,
            'email': email,
            'model_name': model_name,
        }
        try:
            response = requests.post(url, files=files, data=data, verify=True)
            response.raise_for_status()
            response_json = response.json()
            return response_json.get("job_id")
        except Exception as e:
            # FIXME: If the http return code is 400, we should return "invalid_file" instead of None. If the destination is not reachable, we should return "destination_unreachable"
            if response.status_code == 400:
                logging.error(f"[MODEL_TRAINING] Invalid file: {e}")
                return "invalid_file"
            elif response.status_code == 503:
                logging.error(f"[MODEL_TRAINING] Destination unreachable: {e}")
                return "destination_unreachable"
            elif response.status_code == 413:
                logging.error(f"[MODEL_TRAINING] File too large: {e}")
                return "file_too_large"
            elif response.status_code == 500:
                logging.error(f"[MODEL_TRAINING] Internal server error: {e}")
                return "internal_server_error"
            elif response.status_code == 404:
                logging.error(f"[MODEL_TRAINING] Destination not found: {e}")
                return "destination_not_found"
            else:
                logging.error(f"[MODEL_TRAINING] Unknown error: {e}")
                return "unknown_error"
        finally:
            files['file'].close()

    @staticmethod
    def get_model_training_status(job_id):
        """
        Checks the status of a model training job.
        Returns status info on success.
        """
        url = f"{RemoteModelTrainer.BASE_URL}/status/{job_id}"
        try:
            response = requests.get(url, verify=True)
            if response.status_code == 404:
                logging.error(f"[MODEL_TRAINING] Job {job_id} not found.")
                return None
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"[MODEL_TRAINING] Error checking status: {e}")
            return None
        
    @staticmethod
    def cancel_model_training(job_id):
        """
        Cancels a pending model training job.
        Returns True if the cancellation was successful, False otherwise.
        """
        url = f"{RemoteModelTrainer.BASE_URL}/cancel/{job_id}"
        try:
            response = requests.post(url, verify=True)
            if response.status_code == 404:
                logging.error(f"[MODEL_TRAINING] Job {job_id} not found for cancellation.")
                return False
            response.raise_for_status()
            return True
        except Exception as e:
            logging.error(f"[MODEL_TRAINING] Error cancelling job: {e}")
            return False
    

    @staticmethod
    def download_model(result_id: str, model_name="", token: str = None):
        """
        Downloads the trained model zip file from the remote server,
        extracts the data to /root/models/yolo/<model_name>/.
        If model_name is not provided, the model_name of the info.json will be used. If this is 
        also not available, it is set to the current timestamp as YYYY-MM-DD_HH-MM-SS.
        If the target directory already exists, a unique name is generated by appending a number.
        """

        # Sanitize model_name to avoid file system issues
        def sanitize_directory_name(name):
            # Convert to lowercase
            name = name.lower()
            # Allow only alphanumeric characters, underscores, and hyphens
            sanitized = re.sub(r'[^a-zA-Z0-9_-]', '_', name)
            # Replace subsequent underscores with a single underscore
            sanitized = re.sub(r'_{2,}', '_', sanitized)
            # Remove leading/trailing spaces and dots (though regex already handles this)
            sanitized = sanitized.strip().strip('.')
            # Ensure we have a valid name, default if entirely invalid
            if not sanitized:
                sanitized = "model_" + datetime.now(get_timezone()).strftime("%Y%m%d_%H%M%S")
            return sanitized

        # Download the zip file first, then determine model_name priority
        url = f"{RemoteModelTrainer.BASE_URL}/download/{result_id}"
        headers = {}
        if token:
            headers['token'] = token
        try:
            response = requests.get(url, headers=headers, stream=True, verify=True)
            if response.status_code == 404:
                logging.error(f"[MODEL_TRAINING] Result {result_id} not found.")
                return False
            response.raise_for_status()
            # Read the zip file into memory
            zip_bytes = BytesIO()
            for chunk in response.iter_content(chunk_size=8192):
                zip_bytes.write(chunk)
            zip_bytes.seek(0)

            # Try to read info.json from the zip file if present
            info_json_model_name = None
            with zipfile.ZipFile(zip_bytes) as zf:
                if 'info.json' in zf.namelist():
                    try:
                        with zf.open('info.json') as info_file:
                            info_data = json.load(info_file)
                            info_json_model_name = info_data.get('MODEL_NAME')
                            try:
                                # Parse timestamp from info.json
                                timestamp = datetime.fromisoformat(info_data['TIMESTAMP_UTC'])
                                creation_date = timestamp.astimezone(get_timezone()).strftime("%Y-%m-%d_%H:%M:%S")
                            except (ValueError, TypeError) as e:
                                logging.warning(f"[MODEL_TRAINING] Invalid TIMESTAMP_UTC format: {e}")
                                # Fallback to current timestamp
                                creation_date = datetime.now(get_timezone()).strftime("%Y-%m-%d_%H-%M-%S")
                    except Exception as e:
                        logging.warning(f"[MODEL_TRAINING] Could not parse info.json: {e}")
                # Reset pointer for extraction
                zip_bytes.seek(0)

            # Set model_name priority: argument > info.json > timestamp
            if not model_name:
                if info_json_model_name:
                    model_name = info_json_model_name
                else:
                    model_name = creation_date

            # Ensure unique target directory
            base_dir = "/root/models/yolo"
            model_name = sanitize_directory_name(model_name)
            target_dir = os.path.join(base_dir, model_name)
            unique_dir = target_dir
            count = 1
            while os.path.exists(unique_dir):
                unique_dir = f"{target_dir}_{count}"
                count += 1
            target_dir = unique_dir

            # Extract model.pt, labels.txt, and info.json (if present)
            os.makedirs(target_dir, exist_ok=True)
            with zipfile.ZipFile(zip_bytes) as zf:
                zf.extractall(target_dir)
            
            # Check for required files
            required_files = ["model.pt", "labels.txt", "info.json", "best_ncnn_model/model.ncnn.bin", "best_ncnn_model/model.ncnn.param"]
            missing_files = [f for f in required_files if not os.path.exists(os.path.join(target_dir, f))]
            if missing_files:
                logging.error(f"[MODEL_TRAINING] Missing required files after extraction: {', '.join(missing_files)}")
                return False
            
            logging.info(f"[MODEL_TRAINING] Model downloaded and extracted to {target_dir}")
            return True
        except Exception as e:
            logging.error(f"[MODEL_TRAINING] Error downloading or extracting model: {e}")
            return False
        
    @staticmethod
    def check_model_training_result(show_notification=True, show_in_progress=False, return_pretty_status=False):
        """
        Checks if a model training is in progress and handles the result.
        If the training is completed, it downloads the model and updates the configuration.
        If the training is not in progress, it returns "not_in_progress".
        If the training is in progress, it returns the training status.
        """
        # Check if a model training is in progress
        if is_valid_uuid4(CONFIG["MODEL_TRAINING"]):
            response = RemoteModelTrainer.get_model_training_status(CONFIG["MODEL_TRAINING"])
            try:
                training_status = response.get("status")
                training_result_id = response.get("result_id")
            except:
                training_status = "unknown"
                training_result_id = ""

            if training_status == "completed":
                success = RemoteModelTrainer.download_model(training_result_id)
                if success:
                    if show_notification:
                        ui.notification_show(_("Model training completed! You can select the new model now in the 'Configuration' section."), duration=30, type="message")
                    # Reset the model training ID
                    CONFIG["MODEL_TRAINING"] = ""
                    update_single_config_parameter("MODEL_TRAINING")
                else:
                    if show_notification:
                        ui.notification_show(_("Model training completed, but the model could not be downloaded. Please retry later."), duration=30, type="error")
            else:
                if show_notification and show_in_progress:
                    ui.notification_show(_("Model training is in progress. Please check back later."), duration=5, type="default")
        else:
            training_status = "not_in_progress"
        
        # Map statuses to user-friendly messages
        pretty_status_messages = {
            "pending": _("The training is pending and will start soon."),
            "queued": _("The training is queued and waiting for resources."),
            "completed": _("The training has been successfully completed."),
            "aborted": _("The training was aborted. Please try again."),
            "unknown": _("The training status is unknown. Please check back later."),
            "not_in_progress": _("No model training is in progress."),
        }

        if return_pretty_status:
            # If the status is not in the mapping, return the original status
            return pretty_status_messages.get(training_status, training_status)
        
        # If not returning pretty status, return the training status directly
        # Return the training status
        return training_status
    
class YoloModel:
    """
    Class to handle YOLO models on the local filesystem.
    """
    BASE_DIR = "/root/models/yolo"

    @staticmethod
    def get_model_list():
        """
        Returns a list of available YOLO models with their creation dates.
        Each model is represented as a dictionary with keys: 'display_name', 'directory', 'creation_date'.
        """
        model_list = []
        if not os.path.exists(YoloModel.BASE_DIR):
            return model_list
        
        for dir_name in os.listdir(YoloModel.BASE_DIR):
            model_path = os.path.join(YoloModel.BASE_DIR, dir_name)
            if os.path.isdir(model_path):
                model_name = dir_name
                
                # Try to read model_name, timestamp and unique_id from info.json if it exists
                info_json_path = os.path.join(model_path, "info.json")
                unique_id = None
                if os.path.exists(info_json_path):
                    try:
                        with open(info_json_path, 'r') as f:
                            info_data = json.load(f)
                            model_name = info_data.get('MODEL_NAME', dir_name)
                            unique_id = info_data.get('JOB_ID')
                            model_image_size = info_data.get('MODEL_IMAGE_SIZE', 320)
                            # Read the creation date from info.json
                            try:
                                # Parse timestamp from info.json
                                timestamp = datetime.fromisoformat(info_data['TIMESTAMP_UTC'])
                                creation_date = timestamp.astimezone(get_timezone()).strftime("%Y-%m-%d %H:%M:%S")
                            except (ValueError, TypeError) as e:
                                logging.warning(f"[MODEL] Invalid TIMESTAMP_UTC format for {dir_name}: {e}")
                                # Fallback to file creation time
                                creation_time = os.path.getctime(model_path)
                                creation_date = datetime.fromtimestamp(creation_time, get_timezone()).strftime("%Y-%m-%d %H:%M:%S")
                            
                    except Exception as e:
                        logging.error(f"[MODEL] Could not parse info.json for {dir_name}: {e}")
                
                model_list.append({
                    'display_name': model_name, 
                    'directory': dir_name, 
                    'creation_date': creation_date,
                    'unique_id': unique_id,
                    'full_display_name': f"{model_name} ({creation_date})",
                    'model_image_size': model_image_size
                })
        return model_list
    
    @staticmethod
    def get_model_path(unique_id):
        """
        Returns the path to the model directory based on its unique ID (JOB_ID from info.json).
        If the model is not found, returns None.
        """
        model_list = YoloModel.get_model_list()
        for model in model_list:
            if model.get('unique_id') == unique_id:
                return os.path.join(YoloModel.BASE_DIR, model['directory'])
        return None
    
    @staticmethod
    def get_model_image_size(unique_id):
        """
        Returns the model image size based on its unique ID (JOB_ID from info.json).
        If the model is not found, returns None.
        """
        model_list = YoloModel.get_model_list()
        for model in model_list:
            if model.get('unique_id') == unique_id:
                return model['model_image_size']
        return None
    
    @staticmethod
    def delete_model(unique_id):
        """
        Deletes a YOLO model directory based on its unique ID (JOB_ID from info.json).
        Returns True if the deletion was successful, False otherwise.
        """
        model_list = YoloModel.get_model_list()
        directory_to_delete = None
        
        for model in model_list:
            if model.get('unique_id') == unique_id:
                directory_to_delete = model['directory']
                break
        
        if directory_to_delete:
            model_path = os.path.join(YoloModel.BASE_DIR, directory_to_delete)
            if os.path.exists(model_path):
                try:
                    shutil.rmtree(model_path)  # Using rmtree to delete non-empty directories
                    logging.info(f"[MODEL] Deleted model directory: {model_path}")
                    return True
                except Exception as e:
                    logging.error(f"[MODEL] Error deleting model directory: {e}")
                    return False
            else:
                logging.warning(f"[MODEL] Model directory does not exist: {model_path}")
                return False
        else:
            logging.warning(f"[MODEL] No model found with unique ID: {unique_id}")
            return False
        
    @staticmethod
    def rename_model(unique_id, new_name):
        """
        Renames the MODEL_NAME in the info.json file for the model with the given unique_id (JOB_ID).
        Returns True if the rename was successful, False otherwise.
        """
        model_list = YoloModel.get_model_list()
        directory_to_rename = None

        for model in model_list:
            if model.get('unique_id') == unique_id:
                directory_to_rename = model['directory']
                break

        if directory_to_rename:
            info_json_path = os.path.join(YoloModel.BASE_DIR, directory_to_rename, "info.json")
            if os.path.exists(info_json_path):
                try:
                    with open(info_json_path, 'r') as f:
                        info_data = json.load(f)
                    info_data['MODEL_NAME'] = new_name
                    with open(info_json_path, 'w') as f:
                        json.dump(info_data, f, indent=4)
                    logging.info(f"[MODEL] Renamed model {unique_id} to {new_name} in {info_json_path}")
                    return True
                except Exception as e:
                    logging.error(f"[MODEL] Error renaming model in info.json: {e}")
                    return False
            else:
                logging.warning(f"[MODEL] info.json does not exist for: {directory_to_rename}")
                return False
        else:
            logging.warning(f"[MODEL] No model found with unique ID: {unique_id}")
            return False

class ModelHandler:
    def __init__(self, 
                 model = "tflite",  # Can be instance of TfLite or Yolo
                 modeldir="./tflite/",
                 graph="cv-lite-model.tflite",
                 labelfile="labels.txt",
                 resolution="800x600",
                 framerate=10,
                 jpeg_quality=75,
                 model_image_size=320,
                 num_threads=4):

        self.labelfile = os.path.join(modeldir, labelfile)
        self.model = model
        if self.model == "tflite":
            self.modeldir = modeldir
        else:
            self.modeldir = os.path.join(modeldir, "best_ncnn_model")
        self.graph = graph
        self.resolution = resolution
        self.framerate = framerate
        self.jpeg_quality = jpeg_quality
        self.paused = False
        self.last_log_time = 0
        self.input_size = int(model_image_size)
        self.num_threads = num_threads
        self.cat_names = [cat_name.lower() for cat_name in get_cat_names_list(CONFIG['KITTYHACK_DATABASE_PATH'])]

        # Load the label map
        with open(self.labelfile, 'r') as f:
            self.labels = [line.strip() for line in f.readlines()]
            logging.info(f"[MODEL] Labels loaded: {self.labels}")

    def load_model(self):
        if self.model == "tflite":
            from tflite_runtime.interpreter import Interpreter
            self._Interpreter = Interpreter
            self._yolo = None
            self._model_worker = None

        elif self.model == "yolo":
            from ultralytics import YOLO
            import multiprocessing
            from multiprocessing import Queue
            import os
            from src.baseconfig import configure_logging

            # Check if we're using all available cores
            all_cores = multiprocessing.cpu_count()
            using_all_cores = self.num_threads >= all_cores

            # If using all cores, run the model directly for better performance
            if using_all_cores:
                logging.info(f"[MODEL] Loading YOLO model directly in main process using all available cores")
                self._yolo_model = YOLO(self.modeldir, task="detect", verbose=False)
                
                # Create a wrapper function to match the expected interface
                def direct_inference(frame, input_size):
                    # Run inference directly
                    results = self._yolo_model(frame, stream=True, imgsz=input_size, verbose=False)
                    
                    # Process results
                    mouse_probability = 0
                    own_cat_probability = 0
                    detected_objects = []
                    
                    for r in results:
                        for i, (box, conf, cls) in enumerate(zip(r.boxes.xyxy, r.boxes.conf, r.boxes.cls)):
                            xmin, ymin, xmax, ymax = box
                            object_name = self.labels[int(cls)]
                            probability = float(conf * 100)
                            
                            # Calculate original dimensions from the scale
                            imH, imW = frame.shape[:2]
                            scale = int(input_size) / max(imW, imH)
                            pad_x = (input_size - imW * scale) / 2
                            pad_y = (input_size - imH * scale) / 2
                            
                            # Map bounding box coordinates back to original size
                            xmin_orig = float((xmin - pad_x) / scale)
                            ymin_orig = float((ymin - pad_y) / scale)
                            xmax_orig = float((xmax - pad_x) / scale)
                            ymax_orig = float((ymax - pad_y) / scale)
                            
                            detected_object = {
                                'x': float(xmin_orig / imW * 100),
                                'y': float(ymin_orig / imH * 100),
                                'w': float((xmax_orig - xmin_orig) / imW * 100),
                                'h': float((ymax_orig - ymin_orig) / imH * 100),
                                'name': object_name,
                                'probability': probability
                            }
                            
                            detected_objects.append(detected_object)
                            
                            if object_name.lower() in ["mouse", "maus", "prey", "beute"]:
                                mouse_probability = int(probability)
                            elif object_name.lower() in self.cat_names:
                                own_cat_probability = int(probability)
                    
                    return (mouse_probability, own_cat_probability, detected_objects)
                
                self._yolo = direct_inference
                self._model_worker = None
            else:
                # Use the multiprocessing approach for limited CPU cores
                def model_worker_process(model_path, input_queue, output_queue, num_threads=1):
                    try:
                        # Set CPU affinity for this process based on num_threads
                        import psutil
                        process = psutil.Process()
                        
                        # Calculate which cores to use (0 to num_threads-1)
                        cores_to_use = list(range(num_threads))
                        process.cpu_affinity(cores_to_use)
                        
                        logging.info(f"[MODEL] Worker process running on CPU cores {cores_to_use}")
                        
                        # Load the YOLO model in this process
                        model = YOLO(model_path, task="detect", verbose=False)
                        
                        while True:
                            # Get input from queue
                            job = input_queue.get()
                            if job is None:  # None is our signal to exit
                                break
                                
                            job_id, frame, input_size, pad_x, pad_y, scale, labels, cat_names = job
                            
                            # Perform inference
                            results = model(frame, stream=True, imgsz=input_size)
                            
                            # Process results
                            mouse_probability = 0
                            own_cat_probability = 0
                            detected_objects = []
                            
                            for r in results:
                                logging.info(f"[MODEL] Detected {len(r)} objects in image")
                                for i, (box, conf, cls) in enumerate(zip(r.boxes.xyxy, r.boxes.conf, r.boxes.cls)):
                                    xmin, ymin, xmax, ymax = box
                                    object_name = labels[int(cls)]
                                    probability = float(conf * 100)
                                    
                                    # Map bounding box coordinates back to original size
                                    xmin_orig = float((xmin - pad_x) / scale)
                                    ymin_orig = float((ymin - pad_y) / scale)
                                    xmax_orig = float((xmax - pad_x) / scale)
                                    ymax_orig = float((ymax - pad_y) / scale)
                                    
                                    # Calculate original image dimensions from the scale
                                    imW = int(frame.shape[1] / scale)
                                    imH = int(frame.shape[0] / scale)
                                    
                                    detected_object = {
                                        'x': float(xmin_orig / imW * 100),
                                        'y': float(ymin_orig / imH * 100),
                                        'w': float((xmax_orig - xmin_orig) / imW * 100),
                                        'h': float((ymax_orig - ymin_orig) / imH * 100),
                                        'name': object_name,
                                        'probability': probability
                                    }
                                    
                                    detected_objects.append(detected_object)
                                    
                                    if object_name.lower() in ["mouse", "maus", "prey", "beute"]:
                                        mouse_probability = int(probability)
                                    elif object_name.lower() in cat_names:
                                        own_cat_probability = int(probability)
                            
                            # Return results through the output queue
                            output_queue.put((job_id, mouse_probability, own_cat_probability, detected_objects))
                            
                    except Exception as e:
                        logging.error(f"[MODEL] Worker process error: {e}")
                        import traceback
                        logging.error(traceback.format_exc())
                    finally:
                        logging.info("[MODEL] Worker process exiting")

                # Create the queues for communication
                self._input_queue = Queue()
                self._output_queue = Queue()
                
                # Start the worker process with the specified number of threads
                self._model_worker = multiprocessing.Process(
                    target=model_worker_process,
                    args=(self.modeldir, self._input_queue, self._output_queue, self.num_threads)
                )
                self._model_worker.daemon = True
                self._model_worker.start()
                logging.info(f"[MODEL] Started YOLO worker process using {self.num_threads} CPU cores")

                self._yolo = self._send_to_worker
                self._Interpreter = None
                self._next_job_id = 0
                self._job_results = {}
        else:
            logging.error(f"[MODEL] Unknown model type: {self.model}. Failed to start inference.")
            
    def _send_to_worker(self, frame, input_size):
        """Send a frame to the worker process and return a job ID"""
        job_id = self._next_job_id
        self._next_job_id += 1
        
        # Calculate letterbox parameters
        imH, imW = frame.shape[:2]
        scale = int(input_size) / max(imW, imH)
        pad_x = (input_size - imW * scale) / 2
        pad_y = (input_size - imH * scale) / 2
        
        # Put the job in the queue
        self._input_queue.put((job_id, frame, input_size, pad_x, pad_y, scale, self.labels, self.cat_names))
        return job_id
    
    def _get_result(self, job_id, timeout=1.0):
        """Get the result for a specific job ID"""
        try:
            # Check if we've received the result
            if job_id in self._job_results:
                return self._job_results.pop(job_id)
            
            # Try to get new results from the output queue
            while True:
                try:
                    result_job_id, mouse_prob, cat_prob, objects = self._output_queue.get(timeout=timeout)
                    self._job_results[result_job_id] = (mouse_prob, cat_prob, objects)
                    
                    if result_job_id == job_id:
                        return self._job_results.pop(job_id)
                except:
                    return None
        except Exception as e:
            logging.error(f"[MODEL] Error getting result: {e}")
            return None
    
    def __del__(self):
        """Cleanup when the object is deleted"""
        if hasattr(self, '_model_worker') and self._model_worker and self._model_worker.is_alive():
            self._input_queue.put(None)  # Signal to exit
            self._model_worker.join(timeout=2)  # Give it 2 seconds to exit gracefully

    def run(self):
        """Run the model on the video stream."""
        global videostream

        resW, resH = self.resolution.split('x')
        imW, imH = int(resW), int(resH)

        self.load_model()

        # Check if the model is a YOLO model
        if self.model == "tflite":
            # ------------- TFLite Model -------------
            # Path to .tflite file, which contains the model that is used for object detection
            PATH_TO_TFLITE = os.path.join(self.modeldir, self.graph)

            logging.info(f"[MODEL] Preparing to run TFLite model {PATH_TO_TFLITE} on video stream with resolution {imW}x{imH} @ {self.framerate}fps and quality {self.jpeg_quality}%")

            # Load the Tensorflow Lite model.
            interpreter = self._Interpreter(model_path=PATH_TO_TFLITE, num_threads=self.num_threads)
            interpreter.allocate_tensors()

            # Get model details
            self.tf_input_details = interpreter.get_input_details()
            self.tf_output_details = interpreter.get_output_details()
            self.tf_height = self.tf_input_details[0]['shape'][1]
            self.tf_width = self.tf_input_details[0]['shape'][2]

            self.input_size = max(self.tf_height, self.tf_width)

            self.tf_floating_model = (self.tf_input_details[0]['dtype'] == np.float32)
            logging.info(f"[MODEL] Floating model: {self.tf_floating_model}")
            logging.info(f"[MODEL] Input details: {self.tf_input_details} (model shape: {self.tf_height}x{self.tf_width} --> {self.input_size})")

            # Check output layer name to determine if this model was created with TF2 or TF1,
            # because outputs are ordered differently for TF2 and TF1 models
            self.tf_outname = self.tf_output_details[0]['name']
        
        elif self.model == "yolo":
            # ------------- YOLO Model -------------
            logging.info(f"[MODEL] Preparing to run YOLO model {self.modeldir} on video stream with resolution {imW}x{imH} @ {self.framerate}fps and quality {self.jpeg_quality}%")
            # No need to initialize YOLO here - it's already running in the worker process

        else:
            logging.error(f"[MODEL] Unknown model type: {self.model}. Failed to start inference.") 
            return
        
        # Register task in the sigterm_monitor object
        sigterm_monitor.register_task()

        # Initialize frame rate calculation
        frame_rate_calc = 1
        freq = cv2.getTickFrequency()

        # Initialize video stream
        videostream = VideoStream(resolution=(imW, imH), framerate=self.framerate, jpeg_quality=self.jpeg_quality).start()
        logging.info(f"[CAMERA] Starting video stream...")

        # Wait for the camera to warm up
        detected_objects = []
        frame = None
        stream_start_time = tm.time()
        while frame is None:
            frame = videostream.read_oldest()
            if tm.time() - stream_start_time > 10:
                logging.error("[CAMERA] Camera stream failed to start within 10 seconds!")
                break
            else:
                tm.sleep(0.1)

        if frame is not None:
            logging.info("[CAMERA] Camera stream started successfully.")

        # Calculate padding for letterbox resizing
        scale = int(self.input_size) / max(imW, imH)
        pad_x = (self.input_size - imW * scale) / 2  # Horizontal padding
        pad_y = (self.input_size - imH * scale) / 2  # Vertical padding

        # Flag to ensure we run at least one inference to initialize the model
        first_run = True
        
        while not sigterm_monitor.stop_now:
            # Start timer (for calculating frame rate)
            t1 = cv2.getTickCount()

            # Run at least one inference to initialize the model, even if paused
            if first_run or not self.paused:
                # Grab frame from video stream
                frame = videostream.read_oldest()

                if frame is not None:
                    # Run the CPU intensive model inference only if not paused
                    timestamp = tm.time()

                    if self.model == "tflite":
                        own_cat_probability = 0 # Not supported in the original Kittyflap TFLite models
                        mouse_probability, no_mouse_probability, detected_objects = self._process_frame_tflite(frame, interpreter)
                    elif self.model == "yolo":
                        # Resize the frame to the model input size (keeping aspect ratio)
                        resized_frame = self.letterbox(frame, self.input_size)
                        
                        if hasattr(self, '_model_worker') and self._model_worker:
                            # Using multiprocessing approach
                            job_id = self._yolo(resized_frame, self.input_size)
                            result = self._get_result(job_id)
                            
                            if result:
                                mouse_probability, own_cat_probability, obj_list = result
                                no_mouse_probability = 0
                                
                                detected_objects = []
                                for obj in obj_list:
                                    detected_objects.append(DetectedObject(
                                        obj['x'], obj['y'], obj['w'], obj['h'], obj['name'], obj['probability']
                                    ))
                            else:
                                # No result available yet
                                mouse_probability = 0
                                no_mouse_probability = 0
                                own_cat_probability = 0
                                detected_objects = []
                        else:
                            # Direct inference approach
                            mouse_probability, own_cat_probability, obj_list = self._yolo(resized_frame, self.input_size)
                            no_mouse_probability = 0
                            
                            detected_objects = []
                            for obj in obj_list:
                                detected_objects.append(DetectedObject(
                                    obj['x'], obj['y'], obj['w'], obj['h'], obj['name'], obj['probability']
                                ))
                    
                    if not first_run:
                        image_buffer.append(timestamp, self.encode_jpg_image(frame), None, 
                                            mouse_probability, no_mouse_probability, own_cat_probability, detected_objects=detected_objects)

                    # Calculate framerate
                    t2 = cv2.getTickCount()
                    time1 = (t2 - t1) / freq
                    frame_rate_calc = 1 / time1
                    logging.info(f"[MODEL] Model processing time: {time1:.2f} sec, Frame Rate: {frame_rate_calc:.2f} fps")

                    # Set first_run to False after processing the first frame
                    first_run = False

                else:
                    # Log warning only once every 10 seconds to avoid flooding the log
                    current_time = tm.time()
                    if current_time - self.last_log_time > 10:
                        logging.warning("[CAMERA] No frame received!")
                        self.last_log_time = current_time
            
            # To avoid intensive CPU load, wait here until we reached the desired framerate
            elapsed_time = (cv2.getTickCount() - t1) / freq
            sleep_time = max(0, (1.0 / self.framerate) - elapsed_time)
            tm.sleep(sleep_time)
            
        # Stop the video stream
        videostream.stop()

        sigterm_monitor.signal_task_done()

    def pause(self):
        logging.info("[MODEL] Pausing model processing.")
        self.paused = True

    def resume(self):
        logging.info("[MODEL] Resuming model processing.")
        # Update the list of the cat names
        self.cat_names = [cat_name.lower() for cat_name in get_cat_names_list(CONFIG['KITTYHACK_DATABASE_PATH'])]
        self.paused = False

    def get_run_state(self):
        return not self.paused

    def _process_frame_tflite(self, frame: np.ndarray, interpreter: "Interpreter") -> tuple:
        """
        Process a single frame for object detection using TensorFlow Lite.
        This method handles the preprocessing of the frame, performs inference using the TensorFlow Lite
        interpreter, and processes the detection results.
        Args:
            frame (np.ndarray): Input frame in BGR format
            interpreter (tensorflow.lite.python.interpreter.Interpreter): TFLite interpreter
        Returns:
            tuple: Contains:
                - mouse_probability (float): Probability of mouse detection (0-100)
                - no_mouse_probability (float): Probability of no mouse present (0-100)
                - detected_objects (list): List of DetectedObject instances containing detection results
        """

        input_mean = 127.5
        input_std = 127.5

        if ('StatefulPartitionedCall' in self.tf_outname): # This is a TF2 model
            boxes_idx, classes_idx, scores_idx = 1, 3, 0
        elif ('detected_scores:0' in self.tf_outname):
            boxes_idx, classes_idx, scores_idx = 1, 2, 0
        else: # This is a TF1 model
            boxes_idx, classes_idx, scores_idx = 0, 1, 2
        

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_resized = cv2.resize(frame_rgb, (self.tf_width, self.tf_height))
        input_data = np.expand_dims(frame_resized, axis=0)

        original_h, original_w, __ = frame.shape

        if self.tf_floating_model:
            input_data = (np.float32(input_data) - input_mean) / input_std

        interpreter.set_tensor(self.tf_input_details[0]['index'], input_data)
        interpreter.invoke()

        boxes = interpreter.get_tensor(self.tf_output_details[boxes_idx]['index'])[0]
        classes = interpreter.get_tensor(self.tf_output_details[classes_idx]['index'])[0]
        scores = interpreter.get_tensor(self.tf_output_details[scores_idx]['index'])[0]

        if np.isscalar(scores):
            scores = np.array([scores])
        if np.isscalar(classes):
            classes = np.array([classes])
        if boxes.ndim == 1:
            boxes = np.expand_dims(boxes, axis=0)
        
        no_mouse_probability = 0.0
        mouse_probability = 0.0
        detected_objects = []

        for i in range(len(scores)):
            ymin = int(max(1, (boxes[i][0] * original_h)))
            xmin = int(max(1, (boxes[i][1] * original_w)))
            ymax = int(min(original_h, (boxes[i][2] * original_h)))
            xmax = int(min(original_w, (boxes[i][3] * original_w)))

            object_name = str(self.labels[int(classes[i])])
            probability = float(scores[i] * 100)
            
            detected_objects.append(DetectedObject(
                float(xmin / original_w * 100),
                float(ymin / original_h * 100),
                float((xmax - xmin) / original_w * 100),
                float((ymax - ymin) / original_h * 100),
                object_name,
                probability
            ))

            if object_name == "Maus":
                mouse_probability = int(probability)
            elif object_name == "Keine Maus":
                no_mouse_probability = int(probability)


        return mouse_probability, no_mouse_probability, detected_objects

    def get_camera_frame(self):
        if videostream is not None:
            return videostream.read()
        else:
            logging.error("[CAMERA] 'Get Frame' failed. Video stream is not yet initialized.")
            return None
        
    def encode_jpg_image(self, decoded_image: cv2.typing.MatLike) -> bytes:
        """
        Encodes a decoded image into JPG format.
        Args:
            decoded_image (cv2.typing.MatLike): The image to be encoded, represented as a matrix.
        Returns:
            bytes: The encoded image in JPG format as a byte array.
        """
        # Encode the image as JPG
        _, buffer = cv2.imencode('.jpg', decoded_image)
        blob_data = buffer.tobytes()
        
        return blob_data
    
    def letterbox(self, img, target_size):
        """
        Resize the image to the target size while maintaining the aspect ratio.
        Pads the image with a gray background if necessary.
        Args:
            img (np.ndarray): The input image to be resized.
            target_size (int): The target size for the output image.
        Returns:
            np.ndarray: The resized image with padding if necessary.
        """

        h, w = img.shape[:2]
        scale = target_size / max(h, w)
        new_w, new_h = int(w * scale), int(h * scale)
        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        canvas = np.full((target_size, target_size, 3), 114, dtype=np.uint8)
        top = (target_size - new_h) // 2
        left = (target_size - new_w) // 2
        canvas[top:top+new_h, left:left+new_w] = resized
        return canvas